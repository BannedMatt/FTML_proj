{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e80e0b1",
   "metadata": {},
   "source": [
    "# Exercise 4 regression on a given dataset\n",
    "\n",
    "Perform a regression on the dataset stored in FTML/Project/data/regression/.\n",
    "You are free to choose the regression methods, but you must compare at least two\n",
    "methods. You can do more than 2 but this is not mandatory for this exercise. Discuss the choice of the optimization procedures, solvers, hyperparameters, crossvalidation, etc. The Bayes estimator for this dataset and the squared loss reaches\n",
    "a R2 score of approximately 0.92, for at least 1 of the 2 estimators (1 estimator is\n",
    "enough).\n",
    "\n",
    "Your objective is be to obtain a R2 score superior than 0.88 on the test set, that\n",
    "must not be used during training. Remember that training is the complete model optimisation procedure, including model selection and hyperparameters testing, not\n",
    "only when you call a .fit() method ! This is the topic that we discussed during the\n",
    "practical sessions on train / validation / test and cross-validation. However, since\n",
    "you have the test set, all you can do is \"pretend\" not to use it during training, since\n",
    "you can always compute the score test several times without putting it in your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b0625",
   "metadata": {},
   "source": [
    "We firstly define the librairies we will use, load the data, print some information on it and finally define our target accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a2bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape: X_train (200, 200), y_train (200, 1)\n",
      "test set shape: X_test (200, 200), y_test (200, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the data\n",
    "X_train = np.load(\"../data/regression/X_train.npy\")\n",
    "y_train = np.load(\"../data/regression/y_train.npy\")\n",
    "X_test = np.load(\"../data/regression/X_test.npy\")\n",
    "y_test = np.load(\"../data/regression/y_test.npy\")\n",
    "\n",
    "print(f\"dataset shape: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"test set shape: X_test {X_test.shape}, y_test {y_test.shape}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "TARGET_R2 = 0.88 # as written in the subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de478d2",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning and Model Selection\n",
    "\n",
    "Now we'll tune the hyperparameters for our regression models and find the best performing one. We're comparing five different approaches to see which works best for our dataset.\n",
    "\n",
    "\n",
    "The Linear Regression serves as our baseline because it has no hyperparameters to tune. For Ridge Regression, we test different regularization strengths (alpha values) from 0.001 to 100 to find the right balance between fitting the data and preventing overfitting. Lasso Regression gets similar treatment with alpha values, but we focus on a smaller range since Lasso tends to be more aggressive with feature selection. Elastic Net combines both Ridge and Lasso penalties, so we tune both the alpha parameter and the l1_ratio that controls the mix between the two regularization types.\n",
    "\n",
    "Random Forest is our tree-based model that doesn't need feature scaling. We tune the number of trees (n_estimators), how deep each tree can grow (max_depth), and the minimum samples needed to split a node (min_samples_split).\n",
    "\n",
    "We use 5-fold cross-validation with GridSearchCV to find the best hyperparameters for each model. This means we split our training data into 5 parts, train on 4 parts, and validate on the remaining part, repeating this process 5 times.\n",
    "\n",
    "After we found the best hyperparameters, we'll evaluate all models on our test set to see which one performs best in practice and whether we can achieve our target R² score of 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "612bb92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hyperparameter tuning and choosing the best model...\n",
      "\n",
      "hyper tuning Linear Regression...\n",
      "cross validation R² score: 0.4479 ± 0.1677\n",
      "\n",
      "hyper tuning Ridge Regression...\n",
      "best cross validation R² score: 0.5702\n",
      "best params: {'regressor__alpha': 10.0}\n",
      "\n",
      "hyper tuning Lasso Regression...\n",
      "best cross validation R² score: 0.9250\n",
      "best params: {'regressor__alpha': 0.02}\n",
      "\n",
      "hyper tuning Elastic Net...\n",
      "best cross validation R² score: 0.9130\n",
      "best params: {'regressor__alpha': 0.01, 'regressor__l1_ratio': 0.9}\n",
      "\n",
      "hyper tuning Random Forest...\n",
      "best cross validation R² score: 0.2083\n",
      "best params: {'max_depth': 15, 'min_samples_split': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nhyperparameter tuning and choosing the best model...\")\n",
    "\n",
    "# we define the models and their hyperparameter grids\n",
    "models = {\n",
    "    'Linear Regression': {\n",
    "        'model': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', LinearRegression())\n",
    "        ]),\n",
    "        'params': {}  # No hyperparameters to tune\n",
    "    },\n",
    "    \n",
    "    'Ridge Regression': {\n",
    "        'model': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', Ridge())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'regressor__alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'Lasso Regression': {\n",
    "        'model': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', Lasso(max_iter=2000))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'regressor__alpha': [0.001, 0.01, 0.02, 0.05, 0.1, 0.5, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'Elastic Net': {\n",
    "        'model': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', ElasticNet(max_iter=2000))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'regressor__alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "            'regressor__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 15, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# we will store the best models and their cross-validation results\n",
    "best_models = {}\n",
    "cv_results = {}\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\nhyper tuning {name}...\")\n",
    "    \n",
    "    if config['params']:  # if there are hyperparameters to tune\n",
    "        grid_search = GridSearchCV(\n",
    "            config['model'], \n",
    "            config['params'], \n",
    "            cv=5, \n",
    "            scoring='r2', \n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_models[name] = grid_search.best_estimator_\n",
    "        cv_score = grid_search.best_score_\n",
    "        best_params = grid_search.best_params_\n",
    "        \n",
    "        print(f\"best cross validation R² score: {cv_score:.4f}\")\n",
    "        print(f\"best params: {best_params}\")\n",
    "        \n",
    "    else:  # no hyperparameters to tune, just cross-validate\n",
    "        cv_scores = cross_val_score(config['model'], X_train, y_train, cv=5, scoring='r2')\n",
    "        cv_score = cv_scores.mean()\n",
    "        \n",
    "        config['model'].fit(X_train, y_train)\n",
    "        best_models[name] = config['model']\n",
    "        \n",
    "        print(f\"cross validation R² score: {cv_score:.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    cv_results[name] = cv_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ebfccc",
   "metadata": {},
   "source": [
    "## Final Evaluation on Test Set\n",
    "\n",
    "Now comes the moment of truth - testing our tuned models on the test set that we've kept completely separate from the training process. This is where we'll see how well our models actually perform on unseen data and whether we can achieve our target R² score of 0.88.\n",
    "\n",
    "One of the most important aspects of this evaluation is comparing the cross-validation scores we got during training with the actual test set performance. If there's a big difference between these two, it might indicate that our model is overfitting to the training data. Ideally, we want to see similar performance on both, which would suggest our model generalizes well.\n",
    "\n",
    "We'll identify the best performing model based on the R² score and see if any of our models successfully meet the target threshold. This comparison will also help us understand which approach works best for this particular dataset and whether our hyperparameter tuning was effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c741bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "final Evaluation on the test set\n",
      "\n",
      "Linear Regression:\n",
      "  R2 Score: -9.9240\n",
      "  RMSE: 2.8359\n",
      "  MAE: 2.3420\n",
      "Target R2 < 0.88\n",
      "\n",
      "Ridge Regression:\n",
      "  R2 Score: 0.7153\n",
      "  RMSE: 0.4578\n",
      "  MAE: 0.3665\n",
      "Target R2 < 0.88\n",
      "\n",
      "Lasso Regression:\n",
      "  R2 Score: 0.9231\n",
      "  RMSE: 0.2380\n",
      "  MAE: 0.1944\n",
      "Target R2 > 0.88\n",
      "\n",
      "Elastic Net:\n",
      "  R2 Score: 0.9184\n",
      "  RMSE: 0.2452\n",
      "  MAE: 0.2028\n",
      "Target R2 > 0.88\n",
      "\n",
      "Random Forest:\n",
      "  R2 Score: 0.3456\n",
      "  RMSE: 0.6941\n",
      "  MAE: 0.5373\n",
      "Target R2 < 0.88\n",
      "\n",
      "BEST MODEL: Lasso Regression with R2 = 0.9231\n",
      "\n",
      "cross-validation vs test set results:\n",
      "Linear Regression    | cross_validation: 0.4479 | on test data: -9.9240 | diff: 10.3718\n",
      "Ridge Regression     | cross_validation: 0.5702 | on test data: 0.7153 | diff: 0.1451\n",
      "Lasso Regression     | cross_validation: 0.9250 | on test data: 0.9231 | diff: 0.0019\n",
      "Elastic Net          | cross_validation: 0.9130 | on test data: 0.9184 | diff: 0.0054\n",
      "Random Forest        | cross_validation: 0.2083 | on test data: 0.3456 | diff: 0.1373\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL EVALUATION ON TEST SET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nfinal Evaluation on the test set\")\n",
    "\n",
    "test_results = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    # predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    # calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    test_results[name] = {\n",
    "        'R2': r2,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'RMSE': np.sqrt(mse)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  R2 Score: {r2:.4f}\")\n",
    "    print(f\"  RMSE: {np.sqrt(mse):.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    \n",
    "    # we check if target R2 is achieved\n",
    "    if r2 > TARGET_R2:\n",
    "        print(f\"Target R2 > {TARGET_R2}\")\n",
    "    else:\n",
    "        print(f\"Target R2 < {TARGET_R2}\")\n",
    "\n",
    "# we found the best model \n",
    "best_model_name = max(test_results.keys(), key=lambda x: test_results[x]['R2'])\n",
    "best_r2 = test_results[best_model_name]['R2']\n",
    "\n",
    "print(f\"\\nBEST MODEL: {best_model_name} with R2 = {best_r2:.4f}\")\n",
    "\n",
    "# comparison\n",
    "print(f\"\\ncross-validation vs test set results:\")\n",
    "for name in best_models.keys():\n",
    "    cv_r2 = cv_results[name]\n",
    "    test_r2 = test_results[name]['R2']\n",
    "    print(f\"{name:20s} | cross_validation: {cv_r2:.4f} | on test data: {test_r2:.4f} | diff: {abs(cv_r2-test_r2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca72452f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Looking at our results, we successfully achieved our target R² score of 0.88 on the test set with two of our models. Lasso Regression emerged as the clear winner with an impressive R² score of 0.9231, closely followed by Elastic Net at 0.9184. Both models significantly exceeded our target threshold and came very close to the theoretical Bayes estimator performance of 0.92 mentioned in the exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
